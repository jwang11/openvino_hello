// Copyright (C) 2018-2019 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include <vector>
#include <memory>
#include <string>
#include <ie_blob.h>
#include <inference_engine.hpp>
#include <opencv2/opencv.hpp>

#define UNUSED  __attribute__((unused))

using namespace InferenceEngine;

/**
* @brief Sets image data stored in cv::Mat object to a given Blob object.
* @param orig_image - given cv::Mat object with an image data.
* @param blob - Blob object which to be filled by an image data.
* @param batchIndex - batch index of an image inside of the blob.
*/
template <typename T>
void matU8ToBlob(const cv::Mat& orig_image, InferenceEngine::Blob::Ptr& blob, int batchIndex = 0) {
    InferenceEngine::SizeVector blobSize = blob->getTensorDesc().getDims();
    const size_t width = blobSize[3];
    const size_t height = blobSize[2];
    const size_t channels = blobSize[1];
    T* blob_data = blob->buffer().as<T*>();

    cv::Mat resized_image(orig_image);
    if (static_cast<int>(width) != orig_image.size().width ||
            static_cast<int>(height) != orig_image.size().height) {
        cv::resize(orig_image, resized_image, cv::Size(width, height));
    }

    int batchOffset = batchIndex * width * height * channels;

    for (size_t c = 0; c < channels; c++) {
        for (size_t  h = 0; h < height; h++) {
            for (size_t w = 0; w < width; w++) {
                blob_data[batchOffset + c * width * height + h * width + w] =
                        resized_image.at<cv::Vec3b>(h, w)[c];
            }
        }
    }
}

/**
 * @brief Wraps data stored inside of a passed cv::Mat object by new Blob pointer.
 * @note: No memory allocation is happened. The blob just points to already existing
 *        cv::Mat data.
 * @param mat - given cv::Mat object with an image data.
 * @return resulting Blob pointer.
 */
static UNUSED InferenceEngine::Blob::Ptr wrapMat2Blob(const cv::Mat &mat) {
    size_t channels = mat.channels();
    size_t height = mat.size().height;
    size_t width = mat.size().width;

    size_t strideH = mat.step.buf[0];
    size_t strideW = mat.step.buf[1];

    bool is_dense =
            strideW == channels &&
            strideH == channels * width;

    if (!is_dense) THROW_IE_EXCEPTION
                << "Doesn't support conversion from not dense cv::Mat";

    InferenceEngine::TensorDesc tDesc(InferenceEngine::Precision::U8,
                                      {1, channels, height, width},
                                      InferenceEngine::Layout::NHWC);

    return InferenceEngine::make_shared_blob<uint8_t>(tDesc, mat.data);
}

int main(int argc, char *argv[]) {
    // ------------------------------ Parsing and validation of input args ---------------------------------
    if (argc != 4) {
        std::cout << "Usage : " << argv[0] << " <path_to_model> <path_to_image> <device_name>" << std::endl;
        return EXIT_FAILURE;
    }

    const file_name_t input_model{argv[1]};
    const file_name_t input_image_path{argv[2]};
    const std::string device_name{argv[3]};


    // --------------------------- 1. Load inference engine instance -------------------------------------
    Core ie;

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    CNNNetReader network_reader;
    network_reader.ReadNetwork(fileNameToString(input_model));
    network_reader.ReadWeights(fileNameToString(input_model).substr(0, input_model.size() - 4) + ".bin");
    network_reader.getNetwork().setBatchSize(1);
    CNNNetwork network = network_reader.getNetwork();

    // --------------------------- 3. Configure input & output ---------------------------------------------
    // --------------------------- Prepare input blobs -----------------------------------------------------
    InputInfo::Ptr input_info = network.getInputsInfo().begin()->second;
    std::string input_name = network.getInputsInfo().begin()->first;

    /* Mark input as resizable by setting of a resize algorithm.
     * In this case we will be able to set an input blob of any shape to an infer request.
     * Resize and layout conversions are executed automatically during inference */
    input_info->getPreProcess().setResizeAlgorithm(RESIZE_BILINEAR);
    input_info->setLayout(Layout::NHWC);
    input_info->setPrecision(Precision::U8);

    // --------------------------- Prepare output blobs ----------------------------------------------------
    DataPtr output_info = network.getOutputsInfo().begin()->second;
    std::string output_name = network.getOutputsInfo().begin()->first;

    output_info->setPrecision(Precision::FP32);

    // --------------------------- 4. Loading model to the device ------------------------------------------
    ExecutableNetwork executable_network = ie.LoadNetwork(network, device_name);

    // --------------------------- 5. Create infer request -------------------------------------------------
    InferRequest infer_request = executable_network.CreateInferRequest();

    // --------------------------- 6. Prepare input --------------------------------------------------------
    /* Read input image to a blob and set it to an infer request without resize and layout conversions. */
    cv::Mat image = cv::imread(input_image_path);
    Blob::Ptr imgBlob = wrapMat2Blob(image);  // just wrap Mat data by Blob::Ptr without allocating of new memory
    infer_request.SetBlob(input_name, imgBlob);  // infer_request accepts input blob of any size

    // --------------------------- 7. Do inference --------------------------------------------------------
    /* Running the request synchronously */
    infer_request.Infer();

    // --------------------------- 8. Process output ------------------------------------------------------
    Blob::Ptr output = infer_request.GetBlob(output_name);
    std::vector<unsigned> results;
    TopResults(10, *output, results);

    // Print classification results
    std::cout << std::endl << "Top 10 results:" << std::endl << std::endl;
    for (size_t id = 0; id < 10; ++id) {
        /** Getting probability for resulting class **/
        const auto result = output->buffer().
                    as<InferenceEngine::PrecisionTrait<InferenceEngine::Precision::FP32>::value_type*>()
                    [results[id]];
        std::cout << std::left << results[id] << "\t" << std::fixed << result << std::endl;
    }
    std::cout << std::endl;
    return EXIT_SUCCESS;
}
